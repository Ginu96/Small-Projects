{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A small custome environment with 4 states and 2 actions has been created.*\n",
    "\n",
    "*The model tries to predict the next action agent to should take inorder to increase the rewards.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import gym #library for environment ccreation\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a new customized environment and name it as CustomEnvironment\n",
    "class CustomEnvironment(gym.Env):\n",
    "    \n",
    "    #defining a discrete observation space with four possible states (0,1,2,3) \n",
    "    #and a discrete action space with two possible actions (0,1). \n",
    "    #We also initialize the state variable to None.\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.observation_space=spaces.Discrete(4)  #Three possible states: 0,1,2,3\n",
    "        self.action_space=spaces.Discrete(2)  #Two possible actions: 0,1\n",
    "        self.state = None        \n",
    "        \n",
    "    \n",
    "    #reset method is used to reset the environment to its initial state. Here, \n",
    "    #we randomly sample an initial state from the observation space and set it as the current state. \n",
    "    #We then return the current state as the initial observation.\n",
    "    def reset(self):\n",
    "        self.state=self.observation_space.sample()\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    #The step method is responsible for executing an action in the environment and returning the new state, \n",
    "    #reward, done flag, and additional information. \n",
    "    def step(self, action):\n",
    "        #Perform the given action and update the state\n",
    "        if action==0:\n",
    "            self.state=(self.state + 1) % 3  #Move to the next state\n",
    "        else:\n",
    "            self.state=(self.state - 1) % 3  #Move to the previous state\n",
    "\n",
    "        #Calculate the reward\n",
    "        #compute the reward based on the current state\n",
    "        reward=self._calculate_reward()\n",
    "\n",
    "        #Check if the episode is done\n",
    "        done=False\n",
    "\n",
    "        #Additional information (optional)\n",
    "        info={}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        #Define the reward function based on the current state\n",
    "        if self.state==0:\n",
    "            return 0.5\n",
    "        elif self.state==1:\n",
    "            return 1.0\n",
    "        elif self.state==2:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create an instance of the custom environment and assign it to the env variable. \n",
    "#This instance represents our custom environment.\n",
    "env = CustomEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Reset the environment and get the initial observation\n",
    "#Store the returned initial observation in the observation variable.\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 3, Action: 1, Reward: -0.5, Next state: 2\n",
      "Current state: 2, Action: 0, Reward: 0.5, Next state: 0\n",
      "Current state: 0, Action: 1, Reward: -0.5, Next state: 2\n",
      "Current state: 2, Action: 1, Reward: 1.0, Next state: 1\n",
      "Current state: 1, Action: 0, Reward: -0.5, Next state: 2\n",
      "Current state: 2, Action: 1, Reward: 1.0, Next state: 1\n",
      "Current state: 1, Action: 1, Reward: 0.5, Next state: 0\n",
      "Current state: 0, Action: 0, Reward: 1.0, Next state: 1\n",
      "Current state: 1, Action: 0, Reward: -0.5, Next state: 2\n",
      "Current state: 2, Action: 0, Reward: 0.5, Next state: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Run the interaction loop for a 10 steps\n",
    "for _ in range(10):\n",
    "\n",
    "    #Choose a random action from the given actions \n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    #Perform the action in the environment\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "    #Print the current state, action, reward, and next state\n",
    "    print(f\"Current state: {observation}, Action: {action}, Reward: {reward}, Next state: {next_observation}\")\n",
    "\n",
    "    #Update the current observation\n",
    "    observation = next_observation\n",
    "\n",
    "#Close the environment\n",
    "#Free up any resources associated with the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
