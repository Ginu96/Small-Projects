{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error, median_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data \n",
    "df=pd.read_csv(\"D:\\Beinex\\Python\\Dataset-Kaggle\\Task_26-06\\Salary_Data.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding the data and its distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning & Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the data does not have any unique id or we will not remove duplicates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkking whether there is any na values in the data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding rows where education level is na \n",
    "df[df['Education Level'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows where all values are na\n",
    "df=df.dropna(how='all',axis=0)\n",
    "#checking the na values in data after dropping \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels in eductaion distribution in the data\n",
    "edu=df.value_counts(df['Education Level'])\n",
    "print(edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#found multiple category labels, combined and mapped them\n",
    "map_var={\"High School\":1,\"Bachelor's Degree\":2,\"Bachelor's\":2,\"Master's Degree\":3,\"Master's\":3,\"PhD\":4,\"phD\":4}\n",
    "df['Education'] = df['Education Level'].transform(lambda x: x.map(map_var))\n",
    "df['Education'].unique()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Salary'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing salary considering the job title and eductaion\n",
    "df['Salary']=df.groupby(['Job Title','Education'])['Education'].transform(lambda x: x.fillna(x.mode().iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the rows where still na values are present \n",
    "df.dropna(axis=0,inplace=True)\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the gender distribution in the data\n",
    "gender=df.value_counts(df['Gender'])\n",
    "print(gender)\n",
    "plt.pie(gender,labels=gender,autopct='%1.1f%%')\n",
    "plt.title('Gender distribution in Salary data')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The data has almost an equaal distribution of male and female, however the 'other' categories are found to be significantly less*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The gender labels are changed to numercal values using LabelEncoder method\n",
    "df['Gender']=LabelEncoder().fit_transform(df['Gender']) #male as 1, female 0 and others and 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of unique jobs titles are: \",df['Job Title'].nunique())\n",
    "#The lables under job title are changed to numercal values using LabelEncoder method\n",
    "df['Job Title']=LabelEncoder().fit_transform(df['Job Title']) \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='Years of Experience')\n",
    "plt.title('Distribution of Years of Experience')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Skewed to the right which implies need of normalization before applying ML algorithms as well as the presense of outliers in the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#Normalize the data (age,years of experience)\n",
    "#df[['Age','Years of Experience']]=MinMaxScaler().fit_transform(df[['Age','Years of Experience']])\n",
    "\n",
    "sns.pairplot(df, x_vars=['Age','Gender','Education','Job Title','Years of Experience'], y_vars=[\"Salary\",'Years of Experience'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Strong correlation has been observed between education & salary, age & years of experience*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers in the data\n",
    "sns.boxplot(df['Salary'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No outliers found in salary variable. In case of age and years of experience, some outliers are found above the third quartile. But I believe removing those values might affect the prediction model.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying regression models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the dependent and independent variables \n",
    "X=df[['Age','Gender','Education','Job Title','Years of Experience']]\n",
    "Y=df['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MLR=linear_model.LinearRegression().fit(X,Y)\n",
    "\n",
    "#Multiple linear regression model\n",
    "#Values \n",
    "print('The intercept: ', MLR.intercept_)\n",
    "print('The coefficiants are: ',MLR.coef_)\n",
    "\n",
    "#score\n",
    "print(\"Variability in Y explained by X: \",MLR.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM=ols('Y~X',df).fit()\n",
    "print(LM.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the summary table, it is found that P>|t| value is greater than 0.05 for the variable 'gender', which implies the variable 'gender' is in-significant in predicting the salary*.\n",
    "\n",
    "*Hence, we create another model without 'gender' in feature variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the dependent and independent variables \n",
    "X=df[['Age','Education','Job Title','Years of Experience']]\n",
    "Y=df['Salary']\n",
    "\n",
    "#fitting the model\n",
    "LM=ols('Y~X',df).fit()\n",
    "print(LM.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All variables are found significant with R^2 value 1*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different loss functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Mean Squared Error (MSE)**: Mean Squared Error is a commonly used loss function for regression problems. It calculates the average of the squared differences between the predicted values and the actual values. MSE penalizes larger errors more heavily due to the squaring operation.The lower the MSE value, the better the model's performance, with zero indicating a perfect fit.\n",
    "\n",
    "2) **Mean Absolute Error (MAE)**: Mean Absolute Error calculates the average of the absolute differences between the predicted values and the actual values. MAE treats all errors equally and does not differentiate between small and large errors. The lower the MAE value, the better the model's performance, with zero indicating a perfect fit.\n",
    "\n",
    "3) **R-squared (Coefficient of Determination)**: R-squared is a metric used to measure the proportion of the variance in the target variable that is explained by the model. It represents the goodness of fit of the regression model. R-squared ranges from 0 to 1, where 0 indicates that the model does not explain any variability and 1 indicates a perfect fit.\n",
    "\n",
    "4) **Root Mean Squared Error (RMSE)**:RMSE is the square root of the Mean Squared Error (MSE).It represents the average magnitude of the errors made by the model in the same units as the target variable.RMSE is useful when we want to evaluate the model's performance in a more interpretable scale.\n",
    "\n",
    "5) **Mean Absolute Percentage Error (MAPE)**: MAPE measures the average percentage difference between the predicted and actual values.It calculates the absolute percentage difference for each data point and then takes the average.MAPE is useful when you want to assess the relative error in percentage terms.\n",
    "\n",
    "6) **Huber Loss**: Huber Loss is a combination of MSE and MAE. It behaves like MSE for small errors but switches to MAE for larger errors.\n",
    "Huber Loss is less sensitive to outliers compared to MSE and provides a compromise between MSE and MAE.\n",
    "\n",
    "\n",
    "*In this data, I believe all these listed loss functions are usable. However, since the varaible 'Salary' does not have any outliers Mean Absolute Error and Huber loss function will be the best to understand the performance of the regression based prediction.*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using linear regression model for the analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets (0.7:0.3)\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, train_size=0.7,test_size=0.3, random_state=33)\n",
    "\n",
    "#Create an instance of Linear Regression\n",
    "model=LinearRegression()\n",
    "\n",
    "#Train the Linear Regression model\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "#Evaluate the model using different loss functions\n",
    "mse=mean_squared_error(Y_test,Y_pred)\n",
    "r2=r2_score(Y_test,Y_pred)\n",
    "rmse=np.sqrt(mse)\n",
    "mae=mean_absolute_error(Y_test,Y_pred)\n",
    "mape=np.mean(np.abs((Y_test-Y_pred) / Y_test))*100\n",
    "huber_loss=mean_squared_error(Y_test,Y_pred, squared=False)\n",
    "\n",
    "#Print the loss values\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "print(f\"Huber Loss: {huber_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets (0.8:0.2)\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, train_size=0.8,test_size=0.2, random_state=33)\n",
    "\n",
    "#Create an instance of Linear Regression\n",
    "model=LinearRegression()\n",
    "\n",
    "#Train the Linear Regression model\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "#Evaluate the model using different loss functions\n",
    "mse=mean_squared_error(Y_test,Y_pred)\n",
    "r2=r2_score(Y_test,Y_pred)\n",
    "rmse=np.sqrt(mse)\n",
    "mae=mean_absolute_error(Y_test,Y_pred)\n",
    "mape=np.mean(np.abs((Y_test-Y_pred) / Y_test))*100\n",
    "huber_loss=mean_squared_error(Y_test,Y_pred, squared=False)\n",
    "\n",
    "#Print the loss values\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "print(f\"Huber Loss: {huber_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets (0.9:0.1)\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, train_size=0.9,test_size=0.1, random_state=33)\n",
    "\n",
    "#Create an instance of Linear Regression\n",
    "model=LinearRegression()\n",
    "\n",
    "#Train the Linear Regression model\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "#Evaluate the model using different loss functions\n",
    "mse=mean_squared_error(Y_test,Y_pred)\n",
    "r2=r2_score(Y_test,Y_pred)\n",
    "rmse=np.sqrt(mse)\n",
    "mae=mean_absolute_error(Y_test,Y_pred)\n",
    "mape=np.mean(np.abs((Y_test-Y_pred) / Y_test))*100\n",
    "huber_loss=mean_squared_error(Y_test,Y_pred, squared=False)\n",
    "\n",
    "\n",
    "#Print the loss values\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "print(f\"Huber Loss: {huber_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The model was performing better with a train test split ration of 70:30. Therefore we take this as the best split ratio*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lower loss values indicate better performance in all the loss functions, except for R-squared where higher values indicate better fit. We are considering MSE loss function as the best fit loss fn according to the data.*\n",
    "\n",
    "*The MAE value and Huber loss value corresponding the 70:30 split is* **4.765559110910889e-15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate residuals\n",
    "#Split the data into training and testing sets (0.7:0.3)\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, train_size=0.7,test_size=0.3, random_state=33)\n",
    "\n",
    "#Create an instance of Linear Regression\n",
    "model=LinearRegression()\n",
    "\n",
    "#Train the Linear Regression model\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "\n",
    "Residual=Y_pred-Y_test\n",
    "Residual\n",
    "\n",
    "'''\n",
    "#plotting \n",
    "plt.scatter(Y_pred,Residual,color='green')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
